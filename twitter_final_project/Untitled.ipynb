{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff9d1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "pd.options.mode.chained_assignment = None # no warnings\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import tensorflow, , keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sys\n",
    "sys.path.insert(0, '/home/guinzburg/intelligent_system_tools/twitter_final_project/data')\n",
    "from text_processing import text_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db117a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the CSV files using the pandas read_csv function. we have also dropped the id column from the train set as we wonâ€™t need this for training the model.\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "train = train.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94516d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "ablaze\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "aftershock\n",
      "airplane accident\n",
      "airplane accident\n",
      "airplane accident\n",
      "airplane accident\n",
      "airplane accident\n",
      "airplane accident\n",
      "airplane accident\n",
      "airplane accident\n",
      "airplane accident\n",
      "airplane accident\n",
      "airplane accident\n",
      "airplane accident\n",
      "airplane accident\n",
      "airplane accident\n",
      "airplane accident\n",
      "airplane accident\n",
      "airplane accident\n",
      "airplane accident\n",
      "airplane accident\n"
     ]
    }
   ],
   "source": [
    "train = text_processing(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63cdc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1efe24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\",\n",
    "                           input_shape=[], dtype=tf.string)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e724dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the train data into a training and test set\n",
    "X = train.drop('target', axis=1)\n",
    "y = train['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d464017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_no_id = test.drop('id', axis=1)\n",
    "test_predictions = lr.predict(test_no_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_id = test['id']\n",
    "submission_df_1 = pd.DataFrame({\n",
    "                  \"id\": tweet_id, \n",
    "                  \"target\": test_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2353df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df_1.to_csv('submission_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
